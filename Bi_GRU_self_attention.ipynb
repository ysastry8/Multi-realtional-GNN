{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FuFCEnl06eU"
      },
      "outputs": [],
      "source": [
       #importing the packages, 
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.utils\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import GRU,LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "data = pd.read_excel(\"TV_Reviews.xlsx\")\n",
        "#data=pd.DataFrame(pd.read_csv(\"Cause_Sentiment_500.csv\"))\n",
        "df=data['1'].values\n",
        "print(df)\n",
        "print(df[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuation from causes\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model, Input\n",
        "##bi-gru network for causal extraction\n",
        "class SentenceGetter(object):\n",
        "    \"\"\"Class to Get the sentence in this format:\n",
        "    [(Token_1, Part_of_Speech_1, Tag_1), ..., (Token_n, Part_of_Speech_1, Tag_1)]\"\"\"\n",
        "    def __init__(self, data):\n",
        "        \"\"\"Args:\n",
        "            data is the pandas.DataFrame which contains the above dataset\"\"\"\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w,t) for w, t in zip(s[\"word\"].values.tolist(),\n",
        "\n",
        "                                                           s[\"tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"sentence_id\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "\n",
        "    def get_next(self):\n",
        "        \"\"\"Return one sentence\"\"\"\n",
        "        try:\n",
        "            s = self.grouped[\"sentence:{}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "metadata": {
        "id": "vZvzHerg1Dy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import GRU,LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#data = pd.read_excel(\"TV_Reviews.xlsx\")\n",
        "data=pd.DataFrame(pd.read_excel(\"TV_Reviews.xlsx\"))\n",
        "print(data)\n",
        "getter=SentenceGetter(data)\n",
        "sentences=getter.sentences\n",
        "print(sentences)\n",
        "word= set(data['word'].values)\n",
        "words = list(word)\n",
        "print(words)\n",
        "word2idx = {}\n",
        "word2idx['<PAD>'] = 0\n",
        "word2idx['<UNK>'] = 1\n",
        "for i, word in enumerate(words):\n",
        "  word2idx[word] = i + 1\n",
        "print(word2idx)\n",
        "print(word2idx['any'])\n",
        "max_len = 75\n",
        "n_words = len(word2idx)\n",
        "tags = set(data[\"tag\"].values)\n",
        "print(tags)\n",
        "tags=list(tags)\n",
        "print(tags)\n",
        "n_tags = len(tags)\n",
        "tag2idx={}\n",
        "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2idx['<pad>'] = 0\n",
        "n_tags = len(tag2idx)\n",
        "print(tag2idx)\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "max_len=75\n",
        "X_data = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "y_data = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=0)\n",
        "print(X_data[127])\n",
        "print(y_data[127])\n",
        "## The above denotes data preprocessing\n",
        "MAX_LEN=75\n",
        "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
        "word2idx[\"UNK\"] = 1 # Unknown words\n",
        "word2idx[\"PAD\"] = 0 # Padding\n",
        "\n",
        "# Vocabulary Key:token_index -> Value:word\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
        "# The first entry is reserved for PAD\n",
        "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
        "tag2idx[\"PAD\"] = 0\n",
        "\n",
        "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "\n",
        "# print(\"The word Obama is identified by the index: {}\".format(word2idx[\"Obama\"]))\n",
        "# print(\"The labels B-geo(which defines Geopraphical Enitities) is identified by the index: {}\".format(tag2idx[\"B-geo\"]))\n"
      ],
      "metadata": {
        "id": "nmw75PZ71Hb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# Convert each sentence from list of Token to list of word_index\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "# Padding each sentence to have the same lenght\n",
        "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
        "\n",
        "# Convert Tag/Label to tag_index\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "# Padding each sentence to have the same lenght\n",
        "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "# One-Hot encode\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in y]  # n_tags+1(PAD)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1,random_state=560)\n",
        "X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape\n",
        "#print(y[0])\n",
        "MAX_LEN=75\n",
        "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
        "word2idx[\"UNK\"] = 1 # Unknown words\n",
        "word2idx[\"PAD\"] = 0 # Padding\n",
        "\n",
        "# Vocabulary Key:token_index -> Value:word\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
        "# The first entry is reserved for PAD\n",
        "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
        "tag2idx[\"PAD\"] = 0\n",
        "\n",
        "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "\n",
        "# print(\"The word Obama is identified by the index: {}\".format(word2idx[\"Obama\"]))\n",
        "# print(\"The labels B-geo(which defines Geopraphical Enitities) is identified by the index: {}\".format(tag2idx[\"B-geo\"]))\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# Convert each sentence from list of Token to list of word_index\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "# Padding each sentence to have the same lenght\n",
        "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
        "\n",
        "# Convert Tag/Label to tag_index\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "# Padding each sentence to have the same lenght\n",
        "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "# One-Hot encode\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in y]  # n_tags+1(PAD)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1,random_state=560)\n",
        "X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape\n",
        "#print(y[0])\n"
      ],
      "metadata": {
        "id": "FELUqFsV1RGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "import os\n",
        "import pathlib\n",
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \"/content/drive/MyDrive/glove.840B.300d.txt\"\n",
        ")\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file,encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = ''.join(values[:-300])\n",
        "        coefs = np.asarray(values[-300:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "EMBEDDING_DIM=300\n",
        "num_tokens=n_words+2\n",
        "embedding_matrix = np.zeros((num_tokens, EMBEDDING_DIM))\n",
        "for word, i in word2idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        embedding_matrix[i]=embeddings_index.get('UNK')"
      ],
      "metadata": {
        "id": "jaPO_AEe1Z0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "class BAttention(Layer):\n",
        "    def _init_(self,**kwargs):\n",
        "        super(BAttention,self)._init_(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")\n",
        "        super(BAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        #output=at\n",
        "        return output\n",
        "    #def compute_output_shape(self,input_shape):\n",
        "     #   return (input_shape[0],input_shape[-1])\n",
        "    def get_config(self):\n",
        "        return super(BAttention,self).get_config()\n",
        "EMBEDDING_DIM=300\n",
        "num_tokens=n_words+2\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import GRU,LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "#from keras_contrib.layers import CRF\n",
        "from keras.layers import Embedding\n",
        "import matplotlib.pyplot as plt\n",
        "input = Input(shape=(MAX_LEN,))\n",
        "embedding =Embedding(num_tokens, 300, input_length=MAX_LEN, weights=[embedding_matrix],trainable=False)(input) #\n",
        "model = Bidirectional(GRU(units=150,activation=\"relu\",return_sequences=True,recurrent_dropout=0.1))(embedding)\n",
        "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)\n",
        "model = BAttention()(model)\n",
        "#crf = CRF(n_tags)  # CRF layer, n_tags+1(PAD)\n",
        "out=Dense(4,activation=\"softmax\")(model)\n",
        "#out = crf(model)  # output\n",
        "model = Model(input, out)\n",
        "#model.compile(optimizer=\"rmsprop\",lossmetrics=['crf.accuracy'])\n",
        "#model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RSA_wf8n1a2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_tr, np.array(y_tr), batch_size=10, epochs=8,validation_split=0.1, verbose=2)\n",
        "y_predx=[]\n",
        "for k in pred:\n",
        " y_predx.append(k)\n",
        "y_truex=[]\n",
        "for k in y_te_true:\n",
        "  y_truex.append(k)\n",
        "y_true=y_truex\n",
        "y_pred=y_predx\n",
        "print(y_true)\n",
        "print(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "r=classification_report(y_true,y_pred,target_names=['0','1','2','3'])\n",
        "print(r)\n"
      ],
      "metadata": {
        "id": "lntvW4nF1fnP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
