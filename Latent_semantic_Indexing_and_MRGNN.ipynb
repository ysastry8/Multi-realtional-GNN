{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03_Jyylf1pyw"
      },
      "outputs": [],
      "source": [
        "#Preprocessing the causes \n",
        "from keras.layers import GRU,LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#data = pd.read_excel(\"TV_Reviews.xlsx\")\n",
        "data=pd.DataFrame(pd.read_csv(\"Cause_Sentiment(1).csv\"))\n",
        "df=data['1'].values\n",
        "print(df)\n",
        "print(df[0])\n",
        "stop_words = set(stopwords.words('english'))\n",
        "causes=[]\n",
        "for i in range(0,len(df)):\n",
        "  l=df[i].lower()\n",
        "  l=l.split()\n",
        "  for j in range(0,len(l)):\n",
        "    if ',' in l:\n",
        "     l.remove(',')\n",
        "    if '?' in l:\n",
        "     l.remove('?')\n",
        "  #print(l)\n",
        "  causes.append(l)\n",
        "cause=[]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
"##Preprocessing the causes\n",
        "for i in range(0,len(causes)):\n",
        " l=[]\n",
        " for j in range(0,len(causes[i])):\n",
        "  if causes[i][j] not in stop_words:\n",
        "    l.append(causes[i][j])\n",
        " cause.append(l)\n",
        "print(cause)\n",
        
      ],
      "metadata": {
        "id": "O3FO1Igv4GrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Create tf-idf matrix for the causes\n",
        "import numpy as np\n",
        "word=[]\n",
        "for i in range(0,len(cause)):\n",
        "  for j in range(0,len(cause[i])):\n",
        "    word.append(cause[i][j])\n",
        "words = set(word)\n",
        "words=list(words)\n",
        "word2idx = {}\n",
        "for i, word in enumerate(words):\n",
        "  word2idx[word] = i\n",
        "print(word2idx)\n",
        "n=len(word2idx)\n",
        "print(n)\n",
        "A=[[0]*n]*(len(cause))\n",
        "A=np.array(A)\n",
        "idf=[0]*n\n",
        "idf=np.array(idf)\n",
        "for i in range(0,n):\n",
        " s=0\n",
        " for j in range(0,len(cause)):\n",
        "  s=s+A[j][i]\n",
        "  idf[i]=(1+len(cause))/(1+s)\n",
        "idf=np.log(idf)\n",
        "for i in range(0,n):\n",
        "  idf[i]=idf[i]+1\n",
        "print(A)"
      ],
      "metadata": {
        "id": "LHJQa5OI4NbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
 "##Create tf-idf matrix for the causes\n",
        "for i in range(0,len(cause)):\n",
        "  for j in range(0,len(cause[i])):\n",
        "    A[i][word2idx[cause[i][j]]]= A[i][word2idx[cause[i][j]]]+1\n",
        "idf=[0]*n\n",
        "idf=np.array(idf)\n",
        "for i in range(0,n):\n",
        "  s=0\n",
        "  for j in range(0,len(cause)):\n",
        "   if A[j]\n",
        "  idf[i]=(1+len(cause))/(1+s)\n",
        "idf=np.log(idf)\n",
        "for i in range(0,n):\n",
        "  idf[i]=idf[i]+1\n",
        "for i in range(0,len(A)):\n",
        " for j in range(0,len(A[i])):\n",
        "   A[i][j]=A[i][j]*idf[j]\n",
        "print(A)"
      ],
      "metadata": {
        "id": "RdTEXM384PGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##perform svd decomposition of tf-idf matrix\n",
        "#broad aspects of mobile_reviews=Camera,battery,Memory,Processor,Network,OS\n",
        "num_topics=7            #num_topics=#broad aspects of mobile reviews\n",
        "U,S,V=np.linalg.svd(A)\n",
        "print(U)\n",
        "print(S)\n",
        "print(V)\n",
        "#A=U\n",
        "B=U.T\n",
        "print(B)"
      ],
      "metadata": {
        "id": "_Uj5vjBD4UNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## truncating the U-matrix(topic distribution over documents) as number of topics is less than that of the original U matrix obtained from SVD decomposition of tf-idf matrix\n",
        "U_trunc=[[0.0]*num_topics]*len(cause)\n",
        "U_trunc=np.array(U_trunc)\n",
        "for i in range(0,num_topics):\n",
        "  for j in range(0,len(cause)):\n",
        "    U_trunc[j][i]=B[i][j]"
      ],
      "metadata": {
        "id": "iC0dXxzn4XHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We select the truncated SVD\n",
        "print(U_trunc)\n",
        "print(U_trunc.shape)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.cluster.hierarchy import dendrogram,linkage\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "n=len(A)\n",
        "max_dist=0.78\n",
        "similarity_matrix=cosine_similarity(U_trunc)\n",
        "Z=linkage(similarity_matrix,'ward')\n",
        "cluster_labels=fcluster(Z,max_dist,criterion=\"distance\")\n",
        "cluster_labels=pd.DataFrame(cluster_labels,columns=['ClusterLabel'])\n",
        "print(cluster_labels)\n",
        "print(similarity_matrix)\n",
        "print(len(cluster_labels))\n",
        "##\n",
        "clusterlabels=cluster_labels['ClusterLabel'].values\n",
        "print(clusterlabels)"
      ],
      "metadata": {
        "id": "bf6Y8HgF4aJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "optclusters=set(clusterlabels)\n",
        "print(len(optclusters))\n",
        "##creation of a list where each index represents the cluster number and the elements inside each entry in the list represents the index of the causes\n",
        "clusters=list(optclusters)\n",
        "df=data['2'].values\n",
        "A=[]\n",
        "for i in range(1,len(clusters)+1):\n",
        "  l=[]\n",
        "  for j in range(0,len(cause)):\n",
        "    if clusterlabels[j]==i:\n",
        "      l.append(j)\n",
        "      #y.append(df[j])\n",
        "  A.append(l)\n",
        "  #Y.append(y)\n",
        "print(A)"
      ],
      "metadata": {
        "id": "6nV5nvy54eYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##separating positive, negative and neutral causes from the same cluster and creating sub-clusters\n",
        "##labeling the groups with positive/negative/neutral with the help of causal set\n",
        "from nltk.corpus.reader import ycoe\n",
        "Final_clusters=[]\n",
        "y=[]\n",
        "for i in range(0,len(A)):\n",
        "  POS=[]\n",
        "  NEG=[]\n",
        "  NEU=[]\n",
        "  for j in range(0,len(A[i])):\n",
        "   if df[A[i][j]]=='POSITIVE':\n",
        "     POS.append(A[i][j])\n",
        "   if df[A[i][j]]=='NEGATIVE':\n",
        "     NEG.append(A[i][j])\n",
        "   if df[A[i][j]]=='NEUTRAL':\n",
        "     NEU.append(A[i][j])\n",
        "  if POS!=[]:\n",
        "   Final_clusters.append(POS)\n",
        "   y.append('POSITIVE')\n",
        "  if NEG!=[]:\n",
        "    Final_clusters.append(NEG)\n",
        "    y.append('NEGATIVE')\n",
        "  if NEU!=[]:\n",
        "    Final_clusters.append(NEU)\n",
        "    y.append('NEUTRAL')"
      ],
      "metadata": {
        "id": "wD_u8g7c4hOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Obtaining the initial contextual word embeddings\n",
        "from gensim.models import FastText\n",
        "import os\n",
        "import pathlib\n",
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \"/content/drive/MyDrive/glove.840B.300d.txt\"\n",
        ")\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file,encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = ''.join(values[:-300])\n",
        "        coefs = np.asarray(values[-300:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "EMBEDDING_DIM=300\n",
        "embedding_matrix = np.zeros([len(word2idx), EMBEDDING_DIM])\n",
        "for word, i in word2idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        embedding_matrix[i]=embeddings_index.get('UNK')\n",
        "nodes1=embedding_matrix\n",
        "nodes1=nodes1.tolist()\n",
        "a=nodes1\n",
        "nodes=[]"
      ],
      "metadata": {
        "id": "0XLzZk7H4ktO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##initialising the embeddings for each of the words for each of the causes\n",
        "for i in range(0,len(Final_clusters)):\n",
        "  nodes.append(a)\n",
        "nodes=nodes.tolist()\n",
        "##adjacency matrix preparation\n",
        "import numpy as np\n",
        "adj_matrix=np.array([[[0.0]*300]*len(word2idx)]*len(Final_clusters))\n",
        "for i in range(0,len(Final_clusters)):\n",
        "  a=Final_clusters[i]\n",
        "  for j in range(0,len(a)):\n",
        "    for k in range(0,len(cause[j])):\n",
        "      adj_matrix[i][cause[j][k]][cause[j][k+1]]=1.0\n",
        "adj_matrix=torch.tensor(adj_matrix)"
      ],
      "metadata": {
        "id": "9imH72rM4nyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN_causalembedding:\n",
        "\n",
        "def __init__(self):\n",
        "        self.rnn = torch.nn.GRUCell(out_channels, out_channels, bias=bias)\n",
        "        self.out = Linear(600, 3)      ## final-embeddings will be 600 and number of labels=3\n",
        "def forward(self, nodes,num_layers,adj_matrix,Final_clusters):\n",
        "        \"\"\"\"\"\"\n",
        "        print(\"Forward direction\")\n",
        "        for i in range(0,num_layers):\n",
        "          for j in range(0,len(Final_clusters)):\n",
        "            m = torch.matmul(adj_matrix[j],nodes[j])\n",
        "            print(\"Matrix Multiplication\")\n",
        "            nodes[j] = self.rnn(m, nodes[j])\n",
        "        print(\"Backward direction\")\n",
        "        ##creating the transpose of adjacency matrix\n",
        "        a=[]\n",
        "        for i in range(0,len(Final_clusters)):\n",
        "          a1=adj_matrix[i]\n",
        "          a1=torch.transpose(a1)\n",
        "          a1=a1.tolist()\n",
        "          a.append(a1)\n",
        "        adj_matrix1=torch.tensor(a)\n",
        "        nodes1=nodes\n",
        "        for i in range(0,num_layers):\n",
        "          for j in range(0,num_edges):\n",
        "            m = torch.matmul(adj_matrix1[j],nodes1[j])\n",
        "            print(\"Matrix Multiplication\")\n",
        "            nodes1[j] = self.rnn(m, nodes1[j])\n",
        "        ##concatenating the embeddings obtained from wo adjacency matrices\n",
        "        nodes_final=[]\n",
        "        for i in range(0,len(Final_clusters)):\n",
        "          a1=[]\n",
        "          for j in range(0,len(word2idx)):\n",
        "           a2=nodes[i][j]\n",
        "           a3=nodes1[i][j]\n",
        "           a2=a2.tolist()\n",
        "           a3=a3.tolist()\n",
        "           a4=[]\n",
        "           a4.append(a2)\n",
        "           a4.append(a3)\n",
        "           a1.append(a4)\n",
        "          nodes_final.append(a1)\n",
        "        nodes_final=torch.tensor(nodes_final)\n",
        "        X=[]\n",
        "        O=[]\n",
        "        for i in range(0,len(Final_clusters)):\n",
        "         l=[]  ##list of vectorial representation of the words present in causal cluster\n",
        "         word=[] ##list of word indices belonging to causal cluster\n",
        "         for j in range(0,len(Final_clusters[i])):\n",
        "          for k in range(0,word2idx):\n",
        "            if k in cause[j]:\n",
        "              if k not in word:\n",
        "                word.append[k]\n",
        "                l.append(nodes_final[i][k].tolist());\n",
        "         sum=np.array([0.0]*300);\n",
        "         for m in range(0,len(l)):\n",
        "          sum=sum+l[m]\n",
        "         avg=sum/len(l)\n",
        "         X.append(l)\n",
        "         l1=torch.Tensor(l)\n",
        "         l2=F.softmax(self.out(l1),dim=1)\n",
        "         l2.tolist()\n",
        "         O.append(l2)\n",
        "        O=np.array(O)\n",
        "  return O"
      ],
      "metadata": {
        "id": "zZAdiMgK4qbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
"Evaluation of the model\n",
        "num_layers=100\n",
        "Final-clusters_train,Final-clusters_test,y_train,y_test=train-test-split(Final_clusters,y,test_size=0.33)\n",
        "model = GNN_causalembedding()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "data = data.to(device)\n",
        "# Initialize Optimizer\n",
        "learning_rate = 0.01\n",
        "decay = 5e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "for epoch in range(0, 10000):\n",
        "  train(model)\n",
        "test(Final_cluster_test,y_test)\n",
        "def train(model):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(nodes,num_layers,adj_matrix,Final-clusters_train)\n",
        "  loss = criterion(out, y_train)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "def test(x,y):\n",
        "      model.eval()\n",
        "      out = model(Final-clusters_test, y_test)\n",
        "      # Use the class with highest probability.\n",
        "      pred = out.argmax(dim=1)"
      ],
      "metadata": {
        "id": "lMQbKI4K40KD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
